{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入库：\n",
    "build_efficient_sam_vitt 和 build_efficient_sam_vits：用于构建 EfficientSAM 模型。\n",
    "Image：用于图像处理。\n",
    "transforms：用于将图像转换为张量。\n",
    "torch：PyTorch 库。\n",
    "np：NumPy 库。\n",
    "zipfile：用于解压缩权重文件。\n",
    "加载模型：\n",
    "创建一个字典 models，其中包含一个 EfficientSAM 模型 (efficientsam_ti)。\n",
    "从压缩文件中提取权重文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\EFF_SAM\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] 找不到指定的程序。'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from efficient_sam.build_efficient_sam import build_efficient_sam_vitt, build_efficient_sam_vits\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "import zipfile\n",
    "models = {}\n",
    "models['efficientsam_ti'] = build_efficient_sam_vitt()\n",
    "with zipfile.ZipFile(\"weights/efficient_sam_vits.pt.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"weights\")\n",
    "sample_image_np = np.array(Image.open(\"figs/examples/dog1.jpg\"))\n",
    "sample_image_tensor = transforms.ToTensor()(sample_image_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference函数\n",
    "1. 首先遍历模型对每个模型进行推理\n",
    "2. 然后将图像张量、输入点和标签传入模型、获取预测的掩码和IOU\n",
    "3. 更具IOU分数排序，选择最高的掩码\n",
    "4. 将掩码应用到图像上并保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference():\n",
    "    for model_name, model in models.items():\n",
    "        print('Running inference using ', model_name)\n",
    "        predicted_logits, predicted_iou = model(\n",
    "            sample_image_tensor[None, ...],\n",
    "            input_points.to(torch.float32),\n",
    "            input_labels.to(torch.float32),\n",
    "        )\n",
    "        sorted_ids = torch.argsort(predicted_iou, dim=-1, descending=True)\n",
    "        predicted_iou = torch.take_along_dim(predicted_iou, sorted_ids, dim=2)\n",
    "        predicted_logits = torch.take_along_dim(\n",
    "            predicted_logits, sorted_ids[..., None, None], dim=2\n",
    "        )\n",
    "        # The masks are already sorted by their predicted IOUs.\n",
    "        # The first dimension is the batch size (we have a single image. so it is 1).\n",
    "        # The second dimension is the number of masks we want to generate (in this case, it is only 1)\n",
    "        # The third dimension is the number of candidate masks output by the model.\n",
    "        # For this demo we use the first mask.\n",
    "        mask = torch.ge(predicted_logits[0, 0, 0, :, :], 0).cpu().detach().numpy()\n",
    "        masked_image_np = sample_image_np.copy().astype(np.uint8) * mask[:,:,None]\n",
    "        Image.fromarray(masked_image_np).save(f\"figs/examples/dog1_{model_name}_mask_ui.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on_EVENT_LBUTTONDOWN函数\n",
    "处理鼠标左键点击，当左键点击时，记录点击的坐标，在图像上绘制一个红点并显示点击的坐标\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_EVENT_LBUTTONDOWN(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        xy = \"%d,%d\" % (x, y)\n",
    "        a.append(x)\n",
    "        b.append(y)\n",
    "        cv2.circle(img, (x, y), 1, (0, 0, 255), thickness=-1)\n",
    "        cv2.putText(img, xy, (x, y), cv2.FONT_HERSHEY_PLAIN,\n",
    "                    1.0, (0, 0, 0), thickness=1)\n",
    "        cv2.imshow(\"image\", img)\n",
    " \n",
    "count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主循环初始化：\n",
    "\n",
    "使用 OpenCV 加载和显示图像，等待用户点击。\n",
    "记录点击位置并生成输入点和标签。\n",
    "调用 inference 函数对图像进行掩码生成并保存结果。\n",
    "第一次运行：\n",
    "\n",
    "读取初始图像并显示，等待用户点击两次以记录点坐标。\n",
    "生成输入点和标签，调用 inference 进行处理并保存结果图像。\n",
    "增加 count，避免重复初始化。\n",
    "后续运行：\n",
    "\n",
    "读取处理后的图像，等待用户点击记录新点。\n",
    "根据新点击位置生成输入点和标签，调用 inference 进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference using  efficientsam_ti\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    # 图片路径\n",
    "    model_name = \"efficientsam_ti\"\n",
    "    a = []\n",
    "    b = []\n",
    "    if count == 0:\n",
    "        img = cv2.imread(f\"figs/examples/dog1.jpg\")\n",
    "        cv2.imshow(\"image\", img)\n",
    "        cv2.setMouseCallback(\"image\", on_EVENT_LBUTTONDOWN)\n",
    "        # cv2.imshow(\"image\", img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        positions = [[[[a[0], b[0]], [a[1], b[1]]]]]\n",
    "        input_points = torch.tensor(positions)\n",
    "        sample_image_np = np.array(img)\n",
    "        sample_image_tensor = transforms.ToTensor()(sample_image_np)\n",
    "        input_labels = torch.tensor([[[1, 1]]])\n",
    "        inference()\n",
    "        count += 1\n",
    "        continue\n",
    "    a = []\n",
    "    b = []\n",
    "    img = cv2.imread(f\"figs/examples/dog1_{model_name}_mask.png\")\n",
    "    cv2.namedWindow(\"image\")\n",
    "    cv2.setMouseCallback(\"image\", on_EVENT_LBUTTONDOWN)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    positions = [[[[a[0],b[0]],[a[1],b[1]]]]]\n",
    "    input_points = torch.tensor(positions)\n",
    "    sample_image_np = np.array(img)\n",
    "    sample_image_tensor = transforms.ToTensor()(sample_image_np)\n",
    "    input_labels = torch.tensor([[[1, 1]]])\n",
    "    print(\"---------------------------------------------\")\n",
    "    for i in range(len(a)):\n",
    "        print(a[i], b[i])\n",
    "    inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EFF_SAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
